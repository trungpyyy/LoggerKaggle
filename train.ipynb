{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12240799,"sourceType":"datasetVersion","datasetId":7712784}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Thesis: Training an Adapter for Cruise\n\nThis notebook documents the workflow for training a YOLO-based adapter model tailored for cruise applications. The process includes dataset preparation, configuration file creation, model training, and result management.","metadata":{"id":"aUQ7JVx3Fh9S"}},{"cell_type":"markdown","source":"## Install Required Libraries\n\nIn this step, we will install the necessary libraries for training and evaluation. This includes the `ultralytics` package, which provides the YOLO implementation used in this workflow.","metadata":{"id":"Q95DDc-AFh9V"}},{"cell_type":"code","source":"%%bash\nls /kaggle/input/train-thesis\nmkdir -p dataset\ncp -r /kaggle/input/*/* dataset/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T09:47:47.761482Z","iopub.execute_input":"2025-06-22T09:47:47.761759Z","execution_failed":"2025-06-22T09:48:11.058Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q ultralytics","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-22T09:48:11.058Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the TensorBoard notebook extension\n%load_ext tensorboard\n%tensorboard --logdir models","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-22T09:48:11.058Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ultralytics import YOLO\nimport os\nimport cv2\nimport torch\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport warnings\nimport shutil\nimport random\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n","metadata":{"execution":{"execution_failed":"2025-06-22T09:48:11.058Z"},"trusted":true,"id":"jk1LnuXoFh9X","scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Copy dataset from dataset save with kaggle","metadata":{}},{"cell_type":"markdown","source":"## Create YAML Configuration for Training\n\nThis section describes how to automatically generate a `data.yaml` configuration file required for YOLO training. The script reads class names from `classes.txt`, sets up dataset paths, and writes the configuration in YAML format.","metadata":{"id":"ulebFDeWFh9Y"}},{"cell_type":"code","source":"# Python function to automatically create data.yaml config file\n# 1. Reads \"classes.txt\" file to get list of class names\n# 2. Creates data dictionary with correct paths to folders, number of classes, and names of classes\n# 3. Writes data in YAML format to data.yaml\n\nimport yaml\nimport os\n\ndef create_data_yaml(path_to_classes_txt, path_to_data_yaml):\n\n  # Read class.txt to get class names\n  if not os.path.exists(path_to_classes_txt):\n    print(f'classes.txt file not found! Please create a classes.txt labelmap and move it to {path_to_classes_txt}')\n    return\n  with open(path_to_classes_txt, 'r') as f:\n    classes = []\n    for line in f.readlines():\n      if len(line.strip()) == 0: continue\n      classes.append(line.strip())\n  number_of_classes = len(classes)\n\n  # Create data dictionary\n  data = {\n      'path': 'dataset',\n      'train': 'train/images',\n      'val': 'valid/images',\n      'test': 'test/images',\n      'nc': number_of_classes,\n      'names': classes\n  }\n\n  # Write data to YAML file\n  with open(path_to_data_yaml, 'w') as f:\n    yaml.dump(data, f, sort_keys=False)\n  print(f'Created config file at {path_to_data_yaml}')\n\n  return\n\n# Define path to classes.txt and run function\npath_to_classes_txt = 'dataset/classes.txt'\npath_to_data_yaml = 'data.yaml'\n\ncreate_data_yaml(path_to_classes_txt, path_to_data_yaml)\n\nprint('\\nFile contents:\\n')\n!cat data.yaml","metadata":{"trusted":true,"id":"Qk9o3BDeFh9Z","execution":{"execution_failed":"2025-06-22T09:48:11.058Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Visualization\n\nThis section demonstrates how to visualize the training data with bounding boxes. The code will:\n\n1. Load a random image from the training dataset\n2. Read its corresponding label file\n3. Draw bounding boxes and class labels on the image\n4. Display the annotated image using matplotlib\n\nThis visualization helps verify that:\n- Images are loading correctly\n- Label files are properly formatted\n- Bounding box coordinates are accurate\n- Class IDs are valid\n\nThe visualization uses:\n- OpenCV for image processing and drawing\n- Matplotlib for display\n- Green bounding boxes with class labels\n- RGB color format for proper display\n\nYou can run the next cell to see a random training example with its annotations.\n","metadata":{"id":"0CR4NbxLFh9a"}},{"cell_type":"code","source":"def load_classes(classes_path):\n    \"\"\"\n    Load class names from classes.txt file.\n    \n    Args:\n        classes_path (str): Path to the classes.txt file\n    \n    Returns:\n        list: List of class names\n    \"\"\"\n    if not os.path.exists(classes_path):\n        print(f\"Classes file {classes_path} not found!\")\n        return []\n    \n    with open(classes_path, 'r') as f:\n        classes = [line.strip() for line in f.readlines()]\n    return classes","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-22T09:48:11.059Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_random_samples(data_path, num_samples=6):\n    \"\"\"\n    Visualize random samples from the dataset with bounding boxes in a 2x3 subplot.\n\n    Args:\n        data_path (str): Path to the dataset directory\n        num_samples (int): Number of random samples to visualize (default=6 for 2x3 grid)\n    \"\"\"\n    # Load class names\n    classes_path = os.path.join(data_path, 'classes.txt')\n    class_names = load_classes(classes_path)\n    if not class_names:\n        print(\"No class names loaded, using default class IDs.\")\n    \n    # Load the images and labels\n    images_path = os.path.join(data_path, 'train', 'images')\n    labels_path = os.path.join(data_path, 'train', 'labels')\n\n    # Get list of image files\n    image_files = [f for f in os.listdir(images_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n\n    # Select random images\n    selected_images = random.sample(image_files, min(num_samples, len(image_files)))\n\n    # Create a 2x3 subplot grid\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    axes = axes.ravel()  # Flatten the 2x3 array for easier iteration\n\n    for idx, random_image in enumerate(selected_images):\n        image_path = os.path.join(images_path, random_image)\n        label_path = os.path.join(labels_path, random_image.replace('.jpg', '.txt')\n                                 .replace('.jpeg', '.txt').replace('.png', '.txt'))\n\n        # Read and process the image\n        img = cv2.imread(image_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        # Read labels and draw boxes\n        num_objects = 0\n        if os.path.exists(label_path):\n            with open(label_path, 'r') as f:\n                lines = f.readlines()\n\n            height, width = img.shape[:2]\n            for line in lines:\n                class_id, x_center, y_center, w, h = map(float, line.strip().split())\n                class_id = int(class_id)\n                \n                # Get class name or fallback to class ID\n                class_label = class_names[class_id] if class_id < len(class_names) else f'Class {class_id}'\n\n                # Convert normalized coordinates to pixel coordinates\n                x1 = int((x_center - w/2) * width)\n                y1 = int((y_center - h/2) * height)\n                x2 = int((x_center + w/2) * width)\n                y2 = int((y_center + h/2) * height)\n\n                # Draw rectangle\n                cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n                # Add class label\n                cv2.putText(img, class_label, (x1, y1-10),\n                           cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n                num_objects += 1\n\n        # Display image in subplot\n        axes[idx].imshow(img)\n        axes[idx].set_title(f'Image: {random_image}\\nObjects: {num_objects}')\n        axes[idx].axis('off')\n\n        # Print image details\n        print(f'Image {idx+1}:')\n        print(f'Image shape: {img.shape}')\n        print(f'Image path: {image_path}')\n        print(f'Number of objects detected: {num_objects}\\n')\n\n    # Hide empty subplots if fewer images than num_samples\n    for idx in range(len(selected_images), 6):\n        axes[idx].axis('off')\n        axes[idx].set_visible(False)\n\n    # Adjust layout to prevent overlap\n    plt.tight_layout()\n    plt.show()","metadata":{"id":"NA3AdA5IFh9a","trusted":true,"execution":{"execution_failed":"2025-06-22T09:48:11.059Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_path = 'dataset'\nvisualize_random_samples(data_path, num_samples=6)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-22T09:48:11.059Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Start YOLO model training\n\nThe model and training parameters are defined in the cell below.\nPlease run the next cell to begin training.","metadata":{"id":"hFVe06ylFh9b"}},{"cell_type":"code","source":"# Load pretrained model (better starting point than from scratch)\nmodel = YOLO(\"yolo11n.pt\")  # or \"yolo11n.pt\" for standard YOLOv11","metadata":{"execution":{"execution_failed":"2025-06-22T09:48:11.059Z"},"trusted":true,"id":"xFFlzwtrFh9c","scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Train the model with optimized parameters\nmodel.train(\n    data=\"data.yaml\",       # Path to dataset config file\n    epochs=250,             # Number of training epochs\n    imgsz=640,              # Input image size (square: 640x640)\n    batch=16,               # Batch size (adjust based on GPU memory)\n    device=[0, 1],          # Use GPU 0 and GPU 1\n    patience=50,            # Stop early if no improvement in 50 epochs\n    project=\"models\",\n    optimizer=\"auto\",       # Let YOLO choose the best optimizer\n        lr0=0.005                # Initial learning rate\n)\n","metadata":{"execution":{"execution_failed":"2025-06-22T09:48:11.059Z"},"trusted":true,"id":"DIM0tVfjFh9c","scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training Metrics Analysis\n\nAfter training, we can analyze the model's performance metrics to understand its effectiveness. The following metrics are particularly important:\n\n### Key Performance Indicators\n- **mAP (mean Average Precision)**: Overall detection accuracy\n- **Precision**: Ratio of true positives to all detections\n- **Recall**: Ratio of true positives to all ground truth objects\n- **F1-Score**: Harmonic mean of precision and recall\n\n### Training Progress\n- **Loss Curves**: Monitor training and validation loss\n- **Learning Rate**: Track learning rate adjustments\n- **Confusion Matrix**: Analyze detection errors\n\n### Model Efficiency\n- **Inference Speed**: Frames per second (FPS)\n- **Model Size**: Memory footprint\n- **FLOPs**: Computational complexity\n\nThe metrics will be visualized in the next cell to help evaluate the model's performance.\n","metadata":{"id":"Kqqj5V5IFh9d"}},{"cell_type":"code","source":"# Run validation and get detailed metrics\nmetrics = model.val()\n\n# Extract and print key performance metrics\nprint(\"\\n=== Model Performance Metrics ===\")\nprint(f\"mAP@0.5:        {metrics.box.map50:.4f}\")\nprint(f\"mAP@0.5:0.95:   {metrics.box.map:.4f}\")\nprint(f\"Precision (mp): {metrics.box.mp:.4f}\")\nprint(f\"Recall (mr):    {metrics.box.mr:.4f}\")\nprint(f\"F1-Score (avg): {sum(metrics.box.f1) / len(metrics.box.f1):.4f}\")\n\n# Print per-class metrics safely\nprint(\"\\n=== Per-Class Metrics ===\")\nnum_classes_with_metrics = len(metrics.box.p)\nfor i, cls_name in model.names.items():\n    if i >= num_classes_with_metrics:\n        print(f\"{cls_name}: (no detection results)\")\n        continue\n    precision = metrics.box.p[i]\n    recall = metrics.box.r[i]\n    f1_score = metrics.box.f1[i]\n    print(f\"{cls_name}:\")\n    print(f\"  Precision: {precision:.4f}\")\n    print(f\"  Recall:    {recall:.4f}\")\n    print(f\"  F1-Score:  {f1_score:.4f}\")\n\n# Calculate and print inference speed\nprint(\"\\n=== Inference Speed ===\")\ninference_time = metrics.speed['inference']  # milliseconds per image\nprint(f\"Average inference time: {inference_time:.2f} ms\")\nprint(f\"FPS: {1000 / inference_time:.1f}\")\n","metadata":{"trusted":true,"id":"xF2qK9s0Fh9e","scrolled":true,"execution":{"execution_failed":"2025-06-22T09:48:11.059Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Copy Training Results to Save Server\n\nThis section demonstrates how to securely copy the `runs` directory containing training results to a remote save server. This ensures that your experiment outputs are backed up and accessible for further analysis or sharing.","metadata":{"id":"_5CNVwCjFh9g"}},{"cell_type":"code","source":"!pip install -q gdown\n!gdown 'https://drive.google.com/uc?id=1nQ0_w3uG8McFgPxt-kVS1RPW1aKpb2YS'\n!chmod 400 /kaggle/working/gcp-key\n# !ssh -i /kaggle/working/gcp-key -o StrictHostKeyChecking=no trung@34.142.148.134 \"rm -rf /home/trung/runs\"\n!scp -i /kaggle/working/gcp-key -o StrictHostKeyChecking=no -r models trung@34.142.148.134:/home/trung/models\n!echo \"Done!\"","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-22T09:48:11.059Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test Results Analysis\n\n### Test Images Directory Structure\n\n","metadata":{"id":"S55qE6PYFh9e"}},{"cell_type":"code","source":"from pathlib import Path\nimport cv2\nimport matplotlib.pyplot as plt\nfrom ultralytics import YOLO\n\n# Load model\nmodel = YOLO('models/train/weights/best.pt')\n\n# Define test images directory\ntest_dir = Path('dataset/test/images')\nif not test_dir.exists():\n    print(f\"Test directory {test_dir} not found!\")\n    exit()\n\n# Get all image files\nimage_files = []\nfor ext in ['*.jpg', '*.jpeg', '*.png']:\n    image_files.extend(list(test_dir.glob(ext)))\n\n# Select up to 6 images for visualization\nn_images_to_show = min(6, len(image_files))\nselected_images = image_files[:n_images_to_show]\n\n# Create a 2x3 subplot grid\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()\n\n# Process each selected image\nfor idx, img_path in enumerate(selected_images):\n    # Read and process image\n    img = cv2.imread(str(img_path))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    # Run inference with batch=1\n    results = model.predict(source=img, batch=1, verbose=False)\n    result = results[0]\n\n    # Draw boxes on the image\n    annotated_img = result.plot()\n\n    # Display image\n    axes[idx].imshow(annotated_img)\n    axes[idx].set_title(f'{img_path.name}\\nDetections: {len(result.boxes)}')\n    axes[idx].axis('off')\n\n# Hide any unused subplots\nfor idx in range(len(selected_images), 6):\n    axes[idx].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# === Detection Statistics for all test images ===\nprint(\"\\n=== Detection Statistics ===\")\ntotal_detections = 0\nclass_counts = {}\n\nfor img_path in image_files:\n    results = model.predict(source=str(img_path), batch=1, verbose=False)\n    result = results[0]\n\n    for box in result.boxes:\n        cls = int(box.cls[0].item())  # get class index\n        cls_name = model.names.get(cls, f'class_{cls}')\n        class_counts[cls_name] = class_counts.get(cls_name, 0) + 1\n        total_detections += 1\n\nprint(f\"Total images processed: {len(image_files)}\")\nprint(f\"Total detections: {total_detections}\\n\")\n\nprint(\"Detections per class:\")\nfor cls_name, count in class_counts.items():\n    print(f\"  {cls_name}: {count}\")\n","metadata":{"id":"u0YrQ2RSFh9f","trusted":true,"scrolled":true,"execution":{"execution_failed":"2025-06-22T09:48:11.059Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm -rf models.zip\n!zip -r models.zip models","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-22T09:48:11.059Z"},"scrolled":true},"outputs":[],"execution_count":null}]}